{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bang out a processing routine for whole datafiles, a matrix analysis routine, and then dump them into scripts and get the Google instance grinding on a large sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dicts = []\n",
    "for i in range(2000,2011):\n",
    "    with open('postdata/jf_f12'+str(i)+'.pkl','rb') as cellar:\n",
    "        post_dicts.extend(pickle.load(cellar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_text = []\n",
    "for entry in post_dicts:\n",
    "    post_text.append(entry['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ids = []\n",
    "for entry in post_dicts:\n",
    "    post_ids.append(entry['postid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_posts = []\n",
    "for doc in nlp.pipe(post_text, disable=[\"ner\"]):\n",
    "    doclist = []\n",
    "    for token in doc:\n",
    "        if (token.is_stop==False) & (token.pos_ != 'PUNCT') & (token.like_num==False):\n",
    "            doclist.append(token.lemma_)\n",
    "    parsed_posts.append(' '.join(doclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parsed_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "doc_word_matrix = vectorizer.fit_transform(parsed_posts)\n",
    "doc_word_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(doc_word_matrix.toarray(), \n",
    "             index=post_ids, columns=vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "# Singular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "lsa = TruncatedSVD(3)\n",
    "doc_topic_matrix = lsa.fit_transform(doc_word_matrix)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_matrix = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = ['c_'+str(i) for i in range(3)],\n",
    "             columns = vectorizer.get_feature_names())\n",
    "topic_word_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic\", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    topic_dict = {}\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        word_list = [feature_names[i] for i in topic.argsort()[:-no_top_words-1:-1]]\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            topic_dict[ix]=word_list\n",
    "        else:\n",
    "            topic_dict[topic_names[ix]]=word_list\n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa, vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_range(start_loc=0,num_files=10,topics=3,terms=20):\n",
    "    post_dicts = []\n",
    "    for i in range(start_loc,start_loc+num_files):\n",
    "        filename = 'postdata/jf_f12'+str(i)+'.pkl' \n",
    "        #print('Opening file',filename)\n",
    "        with open(filename,'rb') as cellar:\n",
    "            post_dicts.extend(pickle.load(cellar))\n",
    "    post_text = []\n",
    "    for entry in post_dicts:\n",
    "        post_text.append(entry['post'])\n",
    "    #post_ids = []\n",
    "    #for entry in post_dicts:\n",
    "    #    post_ids.append(entry['postid'])\n",
    "    parsed_posts = []\n",
    "    for doc in nlp.pipe(post_text, disable=[\"ner\"]):\n",
    "        doclist = []\n",
    "        for token in doc:\n",
    "            if (token.is_stop==False) & (token.pos_ != 'PUNCT') & (token.like_num==False):\n",
    "                doclist.append(token.lemma_)\n",
    "        parsed_posts.append(' '.join(doclist))\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    doc_word_matrix = vectorizer.fit_transform(parsed_posts)\n",
    "    lsa = TruncatedSVD(topics)\n",
    "    doc_topic_matrix = lsa.fit_transform(doc_word_matrix)\n",
    "    print(\"Topic ratios: \",lsa.explained_variance_ratio_)\n",
    "    return output_topics(lsa, vectorizer.get_feature_names(), terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_analyze_range(start_loc=0,num_files=10,topics=3,terms=20):\n",
    "    post_dicts = []\n",
    "    for i in range(start_loc,start_loc+num_files):\n",
    "        filename = 'postdata/jf_f12'+str(i)+'.pkl' \n",
    "        #print('Opening file',filename)\n",
    "        with open(filename,'rb') as cellar:\n",
    "            post_dicts.extend(pickle.load(cellar))\n",
    "    post_text = []\n",
    "    for entry in post_dicts:\n",
    "        post_text.append(entry['post'])\n",
    "    #post_ids = []\n",
    "    #for entry in post_dicts:\n",
    "    #    post_ids.append(entry['postid'])\n",
    "    parsed_posts = []\n",
    "    for doc in nlp.pipe(post_text, disable=[\"ner\"]):\n",
    "        doclist = []\n",
    "        for token in doc:\n",
    "            if (token.is_stop==False) & (token.pos_ != 'PUNCT') & (token.like_num==False):\n",
    "                doclist.append(token.lemma_)\n",
    "        parsed_posts.append(' '.join(doclist))\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    doc_word_matrix = vectorizer.fit_transform(parsed_posts)\n",
    "    lsa = TruncatedSVD(topics)\n",
    "    doc_topic_matrix = lsa.fit_transform(doc_word_matrix)\n",
    "    print(\"Topic ratios: \",lsa.explained_variance_ratio_)\n",
    "    display_topics(lsa, vectorizer.get_feature_names(), terms)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(5):\n",
    "    start_record = j*350\n",
    "    display_analyze_range(start_loc=start_record, num_files=50, topics=4, terms=15)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = []\n",
    "for j in range(10):\n",
    "    start_record = j*350\n",
    "    topic_list.append(analyze_range\n",
    "                      (start_loc=start_record, num_files=50, topics=4, terms=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have not tried analyzing the post data with NNMF yet. Let me try comparing those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(3)\n",
    "nmf_doc_topic = nmf_model.fit_transform(doc_word_matrix)\n",
    "nmf_topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = ['c_'+str(i) for i in range(3)],\n",
    "             columns = vectorizer.get_feature_names())\n",
    "nmf_topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_model, vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa, vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't see a significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I may be able to implement my own list of stopwords / trashwords. Let me throw absolutely all my parsed & cleaned words into a massive Counter and study them from most common downward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_post_words = Counter()\n",
    "for doc in nlp.pipe(post_text, disable=[\"ner\"]):\n",
    "    doclist = []\n",
    "    for token in doc:\n",
    "        if (token.is_stop==False) & (token.pos_ != 'PUNCT') & (token.like_num==False):\n",
    "            doclist.append(token.lemma_)\n",
    "    parsed_post_words.update(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_post_words['jeep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_post_words.most_common(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words |= {\"like\",\"post\",\"get\",\"look\",\"go\",\"not\",\"good\",\"need\",\"think\",\"quote\",\n",
    "            \"originally\",\"know\",\"jeep\",\"try\",\"way\",\"want\",\"thing\",\"$\",\"yj\",\"-\",\n",
    "            \"sure\",\"say\",\"bad\",\"/\",\"take\",\"Jeep\",\"u\",\"great\",\"well\",\"tell\",\"be\",\"lot\",\n",
    "            \"have\",\"fine\",\"s\",\"yeah\",\"nice\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = list(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1150, 4161)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_feed = []\n",
    "for doc in nlp.pipe(post_text, disable=[\"ner\"]):\n",
    "    doclist = []\n",
    "    for token in doc:\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        elif token.pos_ == 'PUNCT':\n",
    "            continue\n",
    "        elif token.like_num:\n",
    "            continue\n",
    "        else:\n",
    "            doclist.append(str(token.lemma_))\n",
    "    vect_feed.append(' '.join(doclist))\n",
    "cvect = CountVectorizer(stop_words=stoplist)\n",
    "doc_word_clean = cvect.fit_transform(vect_feed)\n",
    "doc_word_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'jeep' in cvect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(post_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valuable word: alright\n",
      "Punctuation: ,\n",
      "Stopword: -PRON-\n",
      "Valuable word: be\n",
      "Stopword: at\n",
      "Stopword: the\n",
      "Valuable word: point\n",
      "Stopword: be\n",
      "Stopword: -PRON-\n",
      "Valuable word: be\n",
      "Valuable word: get\n",
      "Valuable word: sick\n",
      "Stopword: of\n",
      "Stopword: this\n",
      "Stopword: thing\n",
      "Valuable word: run\n",
      "Stopword: like\n",
      "Valuable word: crap\n",
      "Punctuation: ....\n",
      "Stopword: how\n",
      "Stopword: should\n",
      "Valuable word: stuff\n",
      "Stopword: be\n",
      "Valuable word: hook\n",
      "Stopword: up\n",
      "Punctuation: ?\n",
      "Stopword: -PRON-\n",
      "Valuable word: run\n",
      "Stopword: and\n",
      "Stopword: -PRON-\n",
      "Valuable word: drive\n",
      "Stopword: but\n",
      "Stopword: just\n",
      "Stopword: not\n",
      "Stopword: how\n",
      "Stopword: i\n",
      "Stopword: want\n",
      "Punctuation: ...\n",
      "Stopword: the\n",
      "Valuable word: ignition\n",
      "Valuable word: module\n",
      "Stopword: be\n",
      "Valuable word: disconnected\n",
      "Punctuation: ...\n",
      "Stopword: what\n",
      "Stopword: else\n",
      "Stopword: should\n",
      "Stopword: i\n",
      "Stopword: do\n",
      "Punctuation: ?\n"
     ]
    }
   ],
   "source": [
    "for token in :\n",
    "    if token.is_stop == True:\n",
    "        print(\"Stopword:\",token.lemma_)\n",
    "    elif token.pos_ == 'PUNCT':\n",
    "        print(\"Punctuation:\",token.lemma_)\n",
    "    elif token.like_num == True:\n",
    "        print(\"Number:\",token.lemma_)\n",
    "    else:\n",
    "        print(\"Valuable word:\",token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ugly confusing warning or not, sklearn, at least you cut out some (some!!!) of the stop words I told you to, which is absurdly more than I can say for spaCy.\n",
    "\n",
    "Tomorrow I will have to resume with checking the results of LSA/SVD, NNMF against the results before bonus trash word removal, see if I accomplished anything with all this pain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
